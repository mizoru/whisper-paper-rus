# Робастное распознавание речи за счет крупномасштабного обучения со слабым учителем

## Аннотация

Мы изучаем системы по распознаванию речи, обученные просто предсказывать большое количество расшифровок аудиозаписей из интернета. При увеличении масштаба до 680 000 часов многоязычного и многозадачного обучения с учителем модели показывают хороший уровень обобщенности (генерализации) на стандартных бенчмарках и часто способны соревноваться с предыдущими моделями, целиком обученными с учителем, в условиях предсказания с нулевым выстрелом (zero-shot) без нужды в дообучении. Точность и робастность моделей приближаются к человеческим. Мы делаем модели и код для инференса открытыми, чтобы они послужили основанием для дальнейшей работы над робастным распознаванием речи.

## 1. Введение

Прогресс в автоматическом распознавании речи был ускорен техниками по предобучению без учителя, хорошим примером является модель Wav2Veс 2.0. За счет того, что эти методы позволяют обучать модели непосредственно на сыром звуке, они могут продуктивно задействовать большие датасеты неразмеченной речи, и обучение было быстро масштабировано до 1 000 000 часов обучающих данных, что сильно превышает типичные для академического датасета для обучения с учителем ~1000 часов. При дообучении на стандартных бенчмарках, этот подход продвинул передовой край науки, особенно в условиях малого количества данных.

Эти предобученные аудио-кодировщики выучивают высококачественное представление речи, но из-за того, что они целиком обучены без учителя, они не обладают таким же качественным декодировщиком для перевода этих представлений в полезный вывод, в связи с этим они нуждаются в стадии дообучения для того, чтобы на самом деле выполнять такую задачу, как распознавание речи. К сожалению, это ограничивает полезность и влияние этих моделей, так как дообучение – это все ещё сложный процесс, для которого нужен умелый практик машинного обучения. В нужде в дообучении есть дополнительный риск. Методы машинного обучения чрезвычайно хорошо находят закономерности в обучающем датасете, которые повышают их производительность на других данных из того же датасета. Однако, некоторые из этих закономерностей хрупки и ненадежны и отсутствуют в других наборах данных и распределениях. Особенно шокирующим пример содержится в работе [Radford et al., (2021)], где задокументирован случай 9.2-процентного увеличения в точности классификации при дообучении модели компьютерного зрения на датасете ImageNet без какого-либо увеличения средней точности классификации тех же самых объектов на 7 других несинтетических датасетах. Модель, достигающая «сверхчеловеческих» результатов при обучении на одном датасете, может допускать множество базовых ошибок на другом датасете, возможно именно из-за того, что она пользуется этими особенностями конкретного датасета, которые незаметны людям.

Эти факты говорят о том, что несмотря на то, что предобучение без учителя сильно увеличило качество аудио-кодировщиков, отсутствие настолько же высококачественного предобученного декодировщика, в купе с рекомендуемой процедурой дообучения на одном датасете, является жизненно важной слабостью, ограничивающей полезность и робастность. Задача системы распознавания речи заключается в том, чтобы стабильно работать в широком спектре условий без нужды в дообучении с учителем декодеровщика для каждого распределения использований.

Как показали исследования [Narayanan et al. (2018), Likhomanenko et al. (2020), Chan et al. (2021)] системы распознавания речи, которые были предобучены *с учителем* на различных датасетах/доменах показывают более высокую робастность и генерализацию на отдельных датасетах, чем модели, обученные на единственном источнике. Эти работы достигают этого за счет совмещения как можно большего количества качественных датасетов для распознавания речи. Однако, сейчас доступно лишь среднее количество таких данных. SpeechStew содержит смесь 7 существующих датасетов и достигает 5140 часов данных для обучения с учителем. Хоть это и не несущественное число, оно все равно крошечно по сравнению с упомянутым 1 000 000 часов неразмеченной речи, использованным в работе [Zhang et al. (2021)].

Осознавая ограничивающий размер существующих качественных расшифрованных датасетов, недавние усилия привели к созданию бóльших датасетов для распознавания речи. Ослабляя требование, чтобы расшифровки были высококачественными и проверенными людьми, [Chen et al. (2021) Galvez et al. (2021)] используют изощренные автоматизированные конвейеры обработки и увеличивают количество данных для обучения распознавания речи со слабым учителем до 10 000 и 30 000 часов более шумных обучающих данных. Такой компромисс между качеством и количеством – это часто правильное решение. Хоть этот вопрос пока что и недоисследован для распознавания речи, недавние работы в компьютерном зрении показывают, что переход от высококачественных краудсорсинговых датасетов, таких как ImageNet к датасетам намного более крупным, но для обучения со слабым учителем значительно увеличивает робастность и генерализацию моделей.

Тем не менее эти датасеты всего в несколько раз крупнее, чем сумма существующих качественных датасетов, и намного меньше, чем датасеты для обучения без учителя. В этой работе мы устраняем этот разрыв, масштабируя распознавание речи со слабым учителем до 680 000 часов размеченных аудиоданных. Мы назвали свой подход Whisper. Мы показываем, что результаты моделей обученных на таком объеме данных хорошо переносятся на существующие датасеты с нулевым выстрелом, убирая нужду в дообучении на отдельных датасетах, чтобы достичь качественных результатов.

Помимо масштаба, наша работа направлена на расширение предобучения со слабым учителем за пределы англоязычного распознавания речи и до многоязычности и многозадачности. Из 680 000 часов аудиозаписей, 117 000 часов покрывают 96 других языков. Также, датасет содержит 125 000 часов данных с переводом с языка Х на английский (далее X→англ.). Мы обнаружили, что достаточно крупные модели не страдают, а даже выигрывают от совмещения многоязычного и многозадачного обучения.

Наша работа показывает, что простое масштабирование предобучения со слабым учителем было до сих пор недооценено для распознавания речи. Мы достигаем этих результатов, не прибегая к техникам самообучения, которые были основой недавних крупномасштабных работ по распознаванию речи. Чтобы послужить основанием для дальнейшей работы над робастным распознаванием речи мы делаем модели и код для инференса открытыми по следующей ссылке: <https://github.com/openai/whisper>.

## 2. Подход

### 2.1. Обработка данных

Следуя тренду недавних работ на использование текста масштаба интернета для создания систем машинного обучения, мы выбираем минималистичный подход к предобработке данных. В отличие от многих работ по распознаванию речи, мы обучаем модели Whsiper предсказывать сырой текст расшифровок без значительной стандартизации, полагаясь на то, что выразительная способность моделей «последовательность к последовательности» (sequence-to-sequence) позволит им выучить соответствие между высказываниями и их записанными формами. Это упрощает конвейер распознавания речи, убирая нужду в отдельном шаге обратной нормализации текста для создания натуральных расшифровок.

Мы собираем датасет из аудиозаписей в интернете, которые имеют парную расшифровку. В результате получается очень разнообразный набор данных, покрывающий широкий спектр аудио из разных окружений, условий записи, от разных говорящих, на разных языках. Тогда как разнообразность в качестве записи может помочь в создании робастной модели, разнообразность в качестве расшифровок не несет в себе подобной пользы. Первоначальный осмотр показал, что число некачественных расшифровок в сыром датасете велико. Для разрешения этой проблемы мы разработали несколько автоматических методов фильтрования, что улучшает качество расшифровок.

Многие расшифровки в интернете созданы не людьми, а существующими системами автоматического распознавания речи. Недавние исследования показали, что обучение на датасетах, состоящих из смеси человеческих и машинно-сгенерированных данных, может существенно ухудшить результаты систем машинного перевода. Для того чтобы избежать этой проблемы, мы разработали много эвристических методов для распознавания и удаления машинно-сгенерированных расшифровок из датасета. Многие существующие системы распознавания речи выводят только ограниченное подмножество письменного языка, что приводит к удалению или нормализации аспектов, которые сложно предсказывать из одного только аудиосигнала, таких как сложная пунктуация (восклицательные и вопросительные знаки, запятые), пробельные элементы форматирования, такие как абзацы, или стилистические аспекты, такие как регистр букв. Расшифровка целиком в верхнем или нижнем регистре с очень малой вероятностью была создана человеком. Хоть многие системы распознавания речи содержат какой-то уровень обратной нормализации текста, она очень часто простая или основана на правилах и все равно может быть распознана по другим не проработанным аспектам, таким как отсутствие запятых.

Также, мы пользуемся аудио-детектором языка, который был создан за счет дообучения модели-прототипа, обученной на прототипной версии датасета, на VoxLingua107 для того, чтобы убедиться, что язык устной речи соответствует языку расшифровки согласно CLD2. Если они не совпадают, мы не включаем пару (аудио, расшифровка) в датасет в качестве обучающего примера. Мы делаем исключение для тех случаев, когда языком расшифровки является английский, и добавляем эти пары к датасету в качестве примеров перевода речи X→англ. Мы используем нечеткую дедупликацию (fuzzy de-duping) текстов расшифровок, чтобы уменьшить количество дубликатов и автоматически сгенерированного содержимого в обучающем датасете.

Мы разбиваем аудиозаписи на фрагменты по 30 секунд в паре с частью расшифровки, которая звучит в этом временном промежутке.  Мы обучаем модель на всем аудио, включая сегменты, в которых нет никакой речи (хоть и с субдискретизационной (sub-sampling) вероятностью), и используем эти фрагменты в качестве обучающих данных для детекции голосовой активности.

Для дополнительной фильтрации, после обучения начальной модели, мы агрегировали информацию о частоте ошибок этой модели на источниках обучающих данных и произвели ручной осмотр этих источников данных, сортируя по частоте ошибок и размеру источника данных, чтобы идентифицировать и удалить низкочастотные источники эффективно. Этот осмотр показал большое количество лишь частично или плохо расшифрованных или плохо синхронизированных/рассинхронизированных расшифровок, а также оставшиеся низкокачественные машинно-сгенерированные субтитры, которые не были распознаны эвристическими фильтрами.

Чтобы избежать загрязнения, мы проводим дедупликацию на уровне расшифровок между обучающими данными и проверочными датасетами, которые, как нам кажется, имеют больший риск содержать совпадения, а именно датасетом TED-LIUM 3.



### 2.2. Модель

Так как целью работы является изучение способностей крупномасштабного предобучения со слабым учителем для распознавания речи, мы используем готовую архитектуру, чтобы не путать наши результаты с улучшением в самой модели. В качестве архитектуры мы выбрали Трансформер кодировщик-декодировщик, так как доподлинно проверено, что он надежно масштабируется. Все аудиозаписи ресемплированы в 16 000 Гц, и 80-канальная Мел-спектрограмма в лог-шкале просчитана на 25-миллисекундных окнах с шагом в 10 миллисекунд. Для нормализации представлений мы производим глобальное шкалирование входных данных так, чтобы они имели значения от -1 до 1 и имели среднее приблизительно равное 0 по всему предобучающему датасету. Кодировщик обрабатывает эти входные представления с помощью маленького основания, состоящего из двух свёрточных слоев с шириной фильтра 3 и функцией активации GELU, второй свёрточный слой имеет шаг равный двум. Синусоидное позиционное кодирование добавляется к выводу основания, после чего применяются трансформерные блоки кодировщика. Трансформер использует предактивационные остаточные блоки, и финальный слой нормализации применяется к выводу кодировщика. Декодеровщик использует выученные векторы позиционного кодирования и связанные (tied) представления токенов на входе и выходе. Кодировщик и декодировщик имеют одну и ту же ширину и количество трансформерных блоков. Рисунок 1 передает архитектуру модели.

Мы используем тот же самый байтовый основанный на кодировании пар байтов (BPE) текстовый токенизатор, что использовался в GPT-2 для исключительно англоязычных моделей, и переобучаем словарь (сохраняя тот же размер) для многоязычных моделей, чтобы избежать излишней фрагментации других языков, так как словарь GPT-2 исключительно английский.

### 2.3. Многозадачный формат

Хоть предсказание слов сказанных в данном фрагменте аудио – это основная часть полной проблемы распознавания речи и было широко изучено, это не единственная её часть. Полноценная система распознавания речи может состоять из многих дополнительных компонентов таких детекция голосовой активности, диаризация и обратная нормализация текста. Эти компоненты часто реализуют раздельно, в результате чего получается относительно сложная система вокруг основной модели распознавания речи. Для уменьшения этой сложности, мы бы хотели, чтобы одна модель производила весь конвейер распознавания речи, а не только основную часть распознавания. Есть много задач, которые могут быть осуществлены на одном и том же аудиосигнале: расшифровка, перевод, детекция голосовой активности, синхронизация и идентификация языка, – вот лишь некоторые примеры. 
